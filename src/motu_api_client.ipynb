{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motu client snippet to fetch data from Copernicus servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet allows to fetch full areas del of data stored in Copernicus servers and delimited by two points **(x<sub>min</sub>, y<sub>min</sub>)** and **(x<sub>max</sub>, y<sub>max</sub>)**, where x<sub>min</sub> and x<sub>man</sub> represents the minimum and maximum longitude. Likewise y<sub>min</sub> and y<sub>max</sub> denotes the minimum and maximum latitude. Others parameters such as date and depth range and the variables to be fetched are required. A full list of all parmeters can be found here: https://github.com/clstoulouse/motu-client-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the source code from https://github.com/d2gex/afar_coper/archive/refs/heads/master.zip, unzip the downloaded file and \"cd\" into it. Then you need to install the required Python libraries to run this snippet. Please note that the following instructions would install such libraries system-wide and would therefore be available for every program with access to your Python path. Open a console - on Windows by typing CMD on your search box - and run the following command:\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "If you have not yet installed Jupyter, then yo need to install it as well as follows:\n",
    "\n",
    "```bash\n",
    "pip install jupyterlab\n",
    "```\n",
    "\n",
    "If you have used Conda to install your Python libraries stick to it rather than using pip. Lastly open this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python configuration section\n",
    "\n",
    "The script below relies on the following folder structure:\n",
    "\n",
    "* `setup.toml`:configuration file that holds information about the service to be consumed on Copernicus servers \n",
    "* `data`: main root folder where the inputs provided to the Copernicus services and the retrieved data will be stored.\n",
    "    * `output/nc`: folder where fetched **nc** files from Copernicus will be stored\n",
    "    * `output/csv`: folder where converted nc files into **csv** will be stored\n",
    "* `motu_calls.log`: logging file used by Motu to store everything it does to fetch the requested data\n",
    "\n",
    "There are some logging configuration that is not relevant for the execution of this script were you not familiar with Python logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup.toml file\n",
    "This file contains information about the Copernicus service such as the product and service you are about to inquiry. You need to modify these settings if you are planning to use another product or service and whether you want to fetch different variables.\n",
    "\n",
    "```toml\n",
    "base_url = \"https://my.cmems-du.eu/motu-web/Motu\" # Copernicus' base url. It hardly ever changes\n",
    "input_filename = \"api_parameters.csv\" # The name of the file in 'data' root folder where the input parameters such as coordinates, date and depth range are provided to Copernicus\n",
    "output_filename = \"result.nc\" # Name of the file for each file downloaded from Copernicus. An offset will be added with the ID from the input_filename (See next section)\n",
    "service_id = \"GLOBAL_MULTIYEAR_PHY_001_030-TDS\" # Name of the service from Copernicus you are interested in\n",
    "product_id = \"cmems_mod_glo_phy_my_0.083_P1D-m\" # Name of the product from Copernicus you have an interest in\n",
    "variables = [\"thetao\", \"zos\"] # Name of the variables of the service you want to fetch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## input_file format\n",
    "\n",
    "Below there is a snapshot of what the expected input dataframe should be. There are 9 columns with the coordinates, depth and date intervals of the area you may want to explore.  A row could have the same area and different depth or date range. Any combination of these variables generating a different dataset is accepted. The script does not check against duplicate rows. The overall idea is that usually you should have a row per area inspected unless you may want to look at different depths for the same area. You may also consider that the date range is too large for the same area and therefore the file too big to be produced, hence you may want to chop it up into more manageable chunks.\n",
    "\n",
    "| ID  | longitude_min  | longitude_max  | latitude_min  | latitude_max  | depth_min  | depth_max  | date_min  | date_max  |\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|  1 |  -8.9875 | 5.98694444  | 35.875  | 42.99055556  | 0.494024992  | 0.494024992  | 31/12/2020 00:00:00  | 01/01/2021 00:00:00  |\n",
    "|  2 |  -7.9875 | 4.98694444  | 36.875  | 43.99055556  | 0.494024992  | 0.494024992  | 01/01/2019 00:00:00  | 02/01/2019 00:00:00  |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \".env\" credentials file\n",
    "\n",
    "The requests performed against the Copernicus servers requires basic authentication credentials, namely a username and password.  These need to be provided as follows:\n",
    "\n",
    "```toml\n",
    "COPERNICUS_USERNAME=<<your_username>>\n",
    "COPERNICUS_PASSWORD=<<your_password>>\n",
    "```\n",
    "\n",
    "You need to replace the words within \"<<\" \">>\" with your own real details.\n",
    "\n",
    "**DO NOT forget to create this file yourself within the root folder of the project and add your credentias in the same fashion as outlined earlier on!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python configuration file\n",
    "\n",
    "Usually this file doesn't require changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import tomli\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from typing import Dict, Any\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from motu_utils import motu_api\n",
    "import xarray as xr\n",
    "\n",
    "file_path = Path.cwd()\n",
    "ROOT_PATH = file_path.parents[0]\n",
    "DATA_PATH = ROOT_PATH / 'data'\n",
    "OUTPUT_PATH = DATA_PATH / 'output'\n",
    "CSV_PATH = OUTPUT_PATH / 'csv'\n",
    "NC_PATH = OUTPUT_PATH / 'nc'\n",
    "\n",
    "dot_env = load_dotenv(ROOT_PATH / '.env')\n",
    "with open(ROOT_PATH / 'setup.toml', mode=\"rb\") as fp:\n",
    "    settings = tomli.load(fp)\n",
    "\n",
    "INPUT_FILENAME = settings['input_filename']\n",
    "OUTPUT_FILENAME = settings['output_filename']\n",
    "COPERNICUS_USERNAME = os.getenv('COPERNICUS_USERNAME')\n",
    "COPERNICUS_PASSWORD = os.getenv('COPERNICUS_PASSWORD')\n",
    "\n",
    "# Log to the output and into a file\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(ROOT_PATH / 'motu_calls.log', mode='w')\n",
    "sh = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('[%(asctime)s] %(levelname)s [%(filename)s.%(funcName)s:%(lineno)d] %(message)s',\n",
    "                              datefmt='%a, %d %b %Y %H:%M:%S')\n",
    "fh.setFormatter(formatter)\n",
    "sh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(sh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined payload motu classes\n",
    "\n",
    "Usually these two classes aren't required changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotuOptions:\n",
    "    def __init__(self, attrs: dict):\n",
    "        super().__setattr__(\"attrs\", attrs)\n",
    "\n",
    "    def __setattr__(self, k, v):\n",
    "        self.attrs[k] = v\n",
    "\n",
    "    def __getattr__(self, k):\n",
    "        try:\n",
    "            return self.attrs[k]\n",
    "        except KeyError:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotuPayloadGenerator:\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, payload: Dict[str, Any], output_filename: str):\n",
    "        self.data = data\n",
    "        self.payload = payload\n",
    "        self.output_filename = output_filename\n",
    "        self.area_details = []\n",
    "\n",
    "    def _process_row(self, row):\n",
    "        tokens = self.output_filename.split(\".\")\n",
    "        product_details = {\n",
    "            'latitude_min': row[\"latitude_min\"],\n",
    "            'longitude_min': row[\"longitude_min\"],\n",
    "            'latitude_max': row[\"latitude_max\"],\n",
    "            'longitude_max': row[\"longitude_max\"],\n",
    "            'depth_min': row[\"depth_min\"],\n",
    "            'depth_max': row[\"depth_max\"],\n",
    "            'out_name': f\"{row['ID']}_{tokens[0]}.{tokens[-1]}\",\n",
    "            'date_min': (datetime.strptime(row['date_min'], '%d/%m/%Y %H:%M:%S')).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'date_max': (datetime.strptime(row['date_max'], '%d/%m/%Y %H:%M:%S')).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        }\n",
    "        product_details.update(self.payload)\n",
    "        return product_details\n",
    "\n",
    "    def run(self):\n",
    "        return {row['ID']: self._process_row(row) for _, row in self.data.iterrows()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nc to Csv converter class\n",
    "\n",
    "Usually this class doesn't require changing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NcToCsv:\n",
    "\n",
    "    def __init__(self, nc_path: Path, csv_path: Path):\n",
    "        self.nc_path = nc_path\n",
    "        self.csv_path = csv_path\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        nc_paths = [f for f in Path(self.nc_path).glob(str('*.nc'))]\n",
    "        for _path in nc_paths:\n",
    "            ds = xr.open_dataset(_path)\n",
    "            df = ds.to_dataframe()\n",
    "            csv_filename = f\"{str(_path.stem)}.csv\"\n",
    "            abs_csv_path = self.csv_path / csv_filename\n",
    "            df.to_csv(abs_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main body where MOTU api calls are requested\n",
    "\n",
    "This is the bit of the script wheren calls to the Copernicus server through Motu client is done. The paradigm is very simple:\n",
    "1. Fetch a row from the inputted excel file and make a request to Copernicus\n",
    "2. The Copernicus server will start preparing the file to download\n",
    "3. The Motu client will keep asking whether the file is ready until it actually is. Then it downloads it on `output/nc` folder.\n",
    "4. Continue to the next row and proceed from 1-3. All steps are documented into the `motu_calls.log`. See that depending on how large is your input_file and how large the areas, you may have to waint a very long time. You don't need to do anything as the motu client will take care of the whole process.\n",
    "5. Lastly, once all files (one per row in your input_file) have been downloaded, the script will convert all of them into csv and will be place in `output/csv`. The name of the file will start by the ID number of the row being processed + the name provided for your out_file in your `setup.toml` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "if __name__ == \"__main__\":\n",
    "    input_data = pd.read_csv(DATA_PATH / INPUT_FILENAME)\n",
    "    common_payload = {# Common information for each request to Copernicus. This infor mis commmon for all request\n",
    "        'motu': settings['base_url'],\n",
    "        \"auth_mode\": 'cas',\n",
    "        'out_dir': str(NC_PATH),\n",
    "        'user': COPERNICUS_USERNAME,\n",
    "        'pwd': COPERNICUS_PASSWORD,\n",
    "        'service_id': settings['service_id'],\n",
    "        'product_id': settings['product_id'],\n",
    "        'variable': settings['variables']\n",
    "    }\n",
    "    payload_generators = MotuPayloadGenerator(input_data, common_payload, OUTPUT_FILENAME) # It generates a custom set of details for each request: i.e, different coordinates\n",
    "    motu_payloads = payload_generators.run()\n",
    "\n",
    "    # Fetch .data from Coperniculs in .nc format\n",
    "    for _id, payload_data in motu_payloads.items():\n",
    "        logger.info(\n",
    "            f\"------> Processing area  for ID = {_id} delimited by ({payload_data['longitude_min']},{payload_data['latitude_min']}) and \"\n",
    "            f\"({payload_data['longitude_max']},{payload_data['latitude_max']})\")\n",
    "        motu_api.execute_request(MotuOptions(payload_data)) # Real call to Copernicus\n",
    "        logger.info(\"-------> END\")\n",
    "\n",
    "    # Convert all nc files to csv\n",
    "    (NcToCsv(NC_PATH, CSV_PATH))()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
